{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from typing import List, Dict, Any, Optional\n",
        "import math\n",
        "import traceback\n",
        "\n",
        "# -------------------------\n",
        "# Config (adjust as needed)\n",
        "# -------------------------\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LEARNING_RATE = 1e-6\n",
        "MAX_LENGTH = 512\n",
        "GRAD_CLIP_NORM = 1.0\n",
        "BATCH_LOG_EVERY = 10\n",
        "\n",
        "# -------------------------\n",
        "# Safe get_batch_logps\n",
        "# -------------------------\n",
        "def get_batch_logps(model, input_ids: torch.Tensor, attention_mask: torch.Tensor, response_start_indices: List[int]) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns sum of log probabilities over response tokens for each batch element.\n",
        "    input_ids: LongTensor [B, L]\n",
        "    attention_mask: LongTensor [B, L]\n",
        "    response_start_indices: list[int] length B; index (0-based) of first response token in unshifted input_ids\n",
        "    \"\"\"\n",
        "    model_output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    # logits: [B, L, V]\n",
        "    logits = model_output.logits\n",
        "\n",
        "    # Shift logits and targets for causal LM: predict token t from logits at t-1\n",
        "    logits = logits[:, :-1, :].float()  # [B, L-1, V] as float32 for numerical stability\n",
        "    targets = input_ids[:, 1:]           # [B, L-1]\n",
        "    attn_shifted = attention_mask[:, 1:] # [B, L-1]\n",
        "\n",
        "    # log-softmax over vocab\n",
        "    log_probs = F.log_softmax(logits, dim=-1)  # [B, L-1, V]\n",
        "\n",
        "    # gather token log probs\n",
        "    token_log_probs = torch.gather(log_probs, dim=-1, index=targets.unsqueeze(-1)).squeeze(-1)  # [B, L-1]\n",
        "\n",
        "    # Build response mask in the full (unshifted) space, then shift it to align with token_log_probs\n",
        "    B, L = input_ids.shape\n",
        "    full_indices = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)  # [B, L]\n",
        "    start_tensor = torch.tensor(response_start_indices, device=input_ids.device).unsqueeze(1)  # [B, 1]\n",
        "\n",
        "    response_mask_full = (full_indices >= start_tensor).float()  # 1 for response tokens, 0 otherwise; shape [B, L]\n",
        "\n",
        "    # Shift mask to align with token_log_probs/targets (since they are input_ids[:,1:] etc.)\n",
        "    response_mask_shifted = response_mask_full[:, 1:]  # [B, L-1]\n",
        "\n",
        "    # Combine with attention mask (only keep non-padding tokens)\n",
        "    final_mask = (response_mask_shifted * attn_shifted).to(token_log_probs.dtype)  # float\n",
        "\n",
        "    # Zero-out non-response positions (keeping negative log_probs for true tokens)\n",
        "    masked_token_log_probs = token_log_probs * final_mask  # [B, L-1]\n",
        "\n",
        "    # Sum across sequence length to produce a scalar log-prob per example\n",
        "    batch_logps = masked_token_log_probs.sum(dim=-1)  # [B]\n",
        "\n",
        "    return batch_logps  # dtype: float32 on device\n",
        "\n",
        "# -------------------------\n",
        "# Example Dataset & collate_fn\n",
        "# -------------------------\n",
        "class PairwisePreferenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Expects list of dicts with keys: 'prompt', 'chosen', 'rejected'\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path):\n",
        "        self.data = []\n",
        "        with open(file_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                if line.strip():                     # skip empty lines\n",
        "                    self.data.append(json.loads(line))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def collate_fn(batch: List[Dict[str, str]]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Returns a dict with lists; trainer will tokenize using the tokenizer so we keep raw strings here.\n",
        "    \"\"\"\n",
        "    prompts = [item['prompt'] for item in batch]\n",
        "    chosen = [p + \"\\n\" + item['chosen'] for p, item in zip(prompts, batch)]\n",
        "    rejected = [p + \"\\n\" + item['rejected'] for p, item in zip(prompts, batch)]\n",
        "    return {\"prompt\": prompts, \"chosen\": chosen, \"rejected\": rejected}\n",
        "\n",
        "# -------------------------\n",
        "# DPO Trainer\n",
        "# -------------------------\n",
        "class CustomDPOTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        policy_model: AutoModelForCausalLM,\n",
        "        ref_model: AutoModelForCausalLM,\n",
        "        tokenizer: AutoTokenizer,\n",
        "        lr: float = LEARNING_RATE,\n",
        "        beta: float = 1.0,\n",
        "        device: torch.device = DEVICE,\n",
        "        max_length: int = MAX_LENGTH,\n",
        "        optimizer: Optional[torch.optim.Optimizer] = None,\n",
        "    ):\n",
        "        self.policy_model = policy_model.to(device)\n",
        "        self.ref_model = ref_model.to(device)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "        self.beta = beta\n",
        "\n",
        "        # freeze ref model\n",
        "        self.ref_model.eval()\n",
        "        for p in self.ref_model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = optimizer if optimizer is not None else optim.AdamW(self.policy_model.parameters(), lr=lr)\n",
        "\n",
        "    def train_epoch(self, dataloader: DataLoader, clip_norm: float = GRAD_CLIP_NORM, scheduler=None):\n",
        "        self.policy_model.train()\n",
        "        total_loss = 0.0\n",
        "        total_batches = 0\n",
        "\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            try:\n",
        "                prompts = batch[\"prompt\"]\n",
        "                chosen_texts = batch[\"chosen\"]\n",
        "                rejected_texts = batch[\"rejected\"]\n",
        "\n",
        "                # Tokenize chosen/rejected using same settings so prompt length computation is consistent\n",
        "                chosen_enc = self.tokenizer(chosen_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length).to(self.device)\n",
        "                rejected_enc = self.tokenizer(rejected_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length).to(self.device)\n",
        "                prompt_enc = self.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length).to(self.device)\n",
        "\n",
        "                # Compute response start indices from prompt_enc attention masks (unshifted)\n",
        "                # This is the index (0-based) of the first response token within each sequence.\n",
        "                response_start_indices = (prompt_enc.attention_mask.sum(dim=1)).tolist()  # list of ints\n",
        "\n",
        "                # Get policy log probs\n",
        "                policy_chosen_logps = get_batch_logps(self.policy_model, chosen_enc.input_ids, chosen_enc.attention_mask, response_start_indices)\n",
        "                policy_rejected_logps = get_batch_logps(self.policy_model, rejected_enc.input_ids, rejected_enc.attention_mask, response_start_indices)\n",
        "\n",
        "                # Get reference log probs (no grad)\n",
        "                with torch.no_grad():\n",
        "                    ref_chosen_logps = get_batch_logps(self.ref_model, chosen_enc.input_ids, chosen_enc.attention_mask, response_start_indices)\n",
        "                    ref_rejected_logps = get_batch_logps(self.ref_model, rejected_enc.input_ids, rejected_enc.attention_mask, response_start_indices)\n",
        "\n",
        "                # Compute log ratios\n",
        "                chosen_log_ratio = policy_chosen_logps - ref_chosen_logps   # [B]\n",
        "                rejected_log_ratio = policy_rejected_logps - ref_rejected_logps  # [B]\n",
        "\n",
        "                logits = self.beta * (chosen_log_ratio - rejected_log_ratio)  # [B]\n",
        "                # Numerically stable: convert to float32\n",
        "                loss = -F.logsigmoid(logits.to(torch.float32)).mean()\n",
        "\n",
        "                # Sanity checks\n",
        "                if torch.isnan(loss) or torch.isinf(loss):\n",
        "                    # print diagnostics\n",
        "                    print(\"NaN/Inf loss detected. Diagnostics:\")\n",
        "                    print(\"policy_chosen_logps:\", policy_chosen_logps)\n",
        "                    print(\"policy_rejected_logps:\", policy_rejected_logps)\n",
        "                    print(\"ref_chosen_logps:\", ref_chosen_logps)\n",
        "                    print(\"ref_rejected_logps:\", ref_rejected_logps)\n",
        "                    raise ValueError(\"Loss is NaN or Inf\")\n",
        "\n",
        "                # Backprop\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                # optional grad clip\n",
        "                if clip_norm is not None:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), clip_norm)\n",
        "                self.optimizer.step()\n",
        "                if scheduler is not None:\n",
        "                    scheduler.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                total_batches += 1\n",
        "\n",
        "                if (step + 1) % BATCH_LOG_EVERY == 0:\n",
        "                    avg = total_loss / max(1, total_batches)\n",
        "                    print(f\"Step {step+1} | avg loss: {avg:.6f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(\"Exception during training step:\", e)\n",
        "                traceback.print_exc()\n",
        "                # don't crash full training; skip this batch\n",
        "                continue\n",
        "\n",
        "        avg_loss = total_loss / max(1, total_batches)\n",
        "        return avg_loss\n",
        "\n",
        "# -------------------------\n",
        "# Example usage\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")            # replace with your tokenizer\n",
        "    policy = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")       # replace with your policy model\n",
        "    ref = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")          # replace with your ref model (copy of policy)\n",
        "\n",
        "    dataset = PairwisePreferenceDataset('financial_rewards.jsonl')\n",
        "    loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    trainer = CustomDPOTrainer(policy, ref, tokenizer)\n",
        "    avg_loss = trainer.train_epoch(loader)\n",
        "    print(\"Avg epoch loss:\", avg_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY-5PudJ3MzS",
        "outputId": "8789325d-79fb-4983-ae63-d902e21e66c2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10 | avg loss: 0.070633\n",
            "Step 20 | avg loss: 0.035388\n",
            "Step 30 | avg loss: 0.023592\n",
            "Step 40 | avg loss: 0.017696\n",
            "Step 50 | avg loss: 0.014157\n",
            "Avg epoch loss: 0.013355556184959698\n"
          ]
        }
      ]
    }
  ]
}